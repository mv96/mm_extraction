{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sealed-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers.git\n",
    "#!pip install transformers datasets\n",
    "#!pip install wandb\n",
    "import warnings\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da02a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_table=pd.read_csv(\"/gpfsdswork/projects/rech/zpf/uyf36me/finetuning_NLP/resources/perfect_pdfs.csv\")\n",
    "#test_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/test_1.csv\"\n",
    "#test_table=pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "christian-turtle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_no</th>\n",
       "      <th>top_left</th>\n",
       "      <th>bot_right</th>\n",
       "      <th>grobid_text</th>\n",
       "      <th>pdf_alto_text</th>\n",
       "      <th>fonts</th>\n",
       "      <th>label</th>\n",
       "      <th>global_fonts</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Superscipt</th>\n",
       "      <th>...</th>\n",
       "      <th>is_Serif</th>\n",
       "      <th>font_color_red</th>\n",
       "      <th>font_color_green</th>\n",
       "      <th>font_color_blue</th>\n",
       "      <th>is_bold_manual</th>\n",
       "      <th>is_italic_manual</th>\n",
       "      <th>is_serif_manual</th>\n",
       "      <th>is_math_manual</th>\n",
       "      <th>new_font_size</th>\n",
       "      <th>pdf_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(200.0, 1009.5555555555555)</td>\n",
       "      <td>(1239.388888888889, 1307.111111111111)</td>\n",
       "      <td>Gabidulin codes [1] can be seen as the analogs...</td>\n",
       "      <td>['Gabidulin[~end_of_font~] codes[~end_of_font~...</td>\n",
       "      <td>['font8  font8  font8  font8  font8  font8  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmti10, cmr10]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(200.0, 1314.4166666666665)</td>\n",
       "      <td>(1239.388888888889, 1544.2222222222224)</td>\n",
       "      <td>For Gabidulin codes, there is no polynomial-ti...</td>\n",
       "      <td>['For[~end_of_font~] Gabidulin[~end_of_font~] ...</td>\n",
       "      <td>['font8  font8  font8  font8  font8  font8  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmmi10, cmr10]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>(200.0, 1551.5277777777776)</td>\n",
       "      <td>(1239.388888888889, 1781.3333333333335)</td>\n",
       "      <td>In this paper, we provide a lower and an upper...</td>\n",
       "      <td>['In[~end_of_font~] this[~end_of_font~] paper,...</td>\n",
       "      <td>['font8  font8  font8  font8  font8  font8  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>(200.0, 492.44444444444446)</td>\n",
       "      <td>(1239.388888888889, 638.8888888888889)</td>\n",
       "      <td>Let q be a power of a prime, let F q denote th...</td>\n",
       "      <td>['Let[~end_of_font~] q[~end_of_font~] be[~end_...</td>\n",
       "      <td>['font8  font10  font8  font8  font8  font8  f...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmmi10, cmr10, msbm10, cmmi8, cmr10, c...</td>\n",
       "      <td>0.792500</td>\n",
       "      <td>0.051528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052639</td>\n",
       "      <td>0.245332</td>\n",
       "      <td>1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>(200.0, 699.8055555555555)</td>\n",
       "      <td>(1239.361111111111, 827.9999999999999)</td>\n",
       "      <td>. The (usual) addition and the non-commutative...</td>\n",
       "      <td>['following[~end_of_font~] holds:[~end_of_font...</td>\n",
       "      <td>['font8  font8  font10  font8  font21  font10 ...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmmi10, cmr10, cmr8, cmmi10, cmr10, cm...</td>\n",
       "      <td>0.940769</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268373</td>\n",
       "      <td>1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529298</th>\n",
       "      <td>31</td>\n",
       "      <td>(225.5123947309031, 548.15097534589)</td>\n",
       "      <td>(1428.3099929919588, 688.0228331059878)</td>\n",
       "      <td>Example 7.20.Consider the TRS R of f(s(x), 0) ...</td>\n",
       "      <td>['Example[~end_of_font~] 7.20.[~end_of_font~] ...</td>\n",
       "      <td>['font28  font28  font4  font4  font4  font13 ...</td>\n",
       "      <td>uri:extthm.example.25</td>\n",
       "      <td>[cmbx10, cmr10, cmsy10, cmr10, cmss10, cmsy10,...</td>\n",
       "      <td>0.928480</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.084770</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.221871</td>\n",
       "      <td>0.267470</td>\n",
       "      <td>1102.3129/arxiv.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529299</th>\n",
       "      <td>31</td>\n",
       "      <td>(225.5123947309031, 699.0814344942514)</td>\n",
       "      <td>(1428.3099929919588, 763.3213500964258)</td>\n",
       "      <td>Here, however, we must not conclude linear run...</td>\n",
       "      <td>['Here,[~end_of_font~] however,[~end_of_font~]...</td>\n",
       "      <td>['font4  font4  font4  font4  font4  font4  fo...</td>\n",
       "      <td>uri:extthm.example.25</td>\n",
       "      <td>[cmr10, cmsy10, cmr10]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1102.3129/arxiv.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529300</th>\n",
       "      <td>31</td>\n",
       "      <td>(225.5123947309031, 899.5533164448088)</td>\n",
       "      <td>(1428.3377757189182, 1227.6714666810017)</td>\n",
       "      <td>All described techniques have been incorporate...</td>\n",
       "      <td>['All[~end_of_font~] described[~end_of_font~] ...</td>\n",
       "      <td>['font4  font4  font4  font4  font4  font4  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmti10, cmss10, cmr10, cmr8, cmr10, cm...</td>\n",
       "      <td>0.985119</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047685</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271632</td>\n",
       "      <td>1102.3129/arxiv.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529301</th>\n",
       "      <td>31</td>\n",
       "      <td>(225.5123947309031, 1238.7300680692654)</td>\n",
       "      <td>(1428.3377757189182, 1566.4870077073494)</td>\n",
       "      <td>Table 1 summarises the experimental results of...</td>\n",
       "      <td>['Table[~end_of_font~] 1[~end_of_font~] summar...</td>\n",
       "      <td>['font4  font4  font4  font4  font4  font4  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmsy10, cmmi10, cmr10, cmss10, cmr10]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1102.3129/arxiv.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529302</th>\n",
       "      <td>31</td>\n",
       "      <td>(225.5123947309031, 1577.5456090956127)</td>\n",
       "      <td>(1428.3377757189182, 1867.653290238478)</td>\n",
       "      <td>However if we consider RMIs upto dimension 3 t...</td>\n",
       "      <td>['However[~end_of_font~] if[~end_of_font~] we[...</td>\n",
       "      <td>['font4  font4  font4  font4  font4  font4  fo...</td>\n",
       "      <td>basic</td>\n",
       "      <td>[cmr10, cmsy10, cmr10, cmmi10, cmr10, cmmi10, ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>1102.3129/arxiv.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>529303 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        page_no                                 top_left  \\\n",
       "0             1              (200.0, 1009.5555555555555)   \n",
       "1             1              (200.0, 1314.4166666666665)   \n",
       "2             1              (200.0, 1551.5277777777776)   \n",
       "3             2              (200.0, 492.44444444444446)   \n",
       "4             2               (200.0, 699.8055555555555)   \n",
       "...         ...                                      ...   \n",
       "529298       31     (225.5123947309031, 548.15097534589)   \n",
       "529299       31   (225.5123947309031, 699.0814344942514)   \n",
       "529300       31   (225.5123947309031, 899.5533164448088)   \n",
       "529301       31  (225.5123947309031, 1238.7300680692654)   \n",
       "529302       31  (225.5123947309031, 1577.5456090956127)   \n",
       "\n",
       "                                       bot_right  \\\n",
       "0         (1239.388888888889, 1307.111111111111)   \n",
       "1        (1239.388888888889, 1544.2222222222224)   \n",
       "2        (1239.388888888889, 1781.3333333333335)   \n",
       "3         (1239.388888888889, 638.8888888888889)   \n",
       "4         (1239.361111111111, 827.9999999999999)   \n",
       "...                                          ...   \n",
       "529298   (1428.3099929919588, 688.0228331059878)   \n",
       "529299   (1428.3099929919588, 763.3213500964258)   \n",
       "529300  (1428.3377757189182, 1227.6714666810017)   \n",
       "529301  (1428.3377757189182, 1566.4870077073494)   \n",
       "529302   (1428.3377757189182, 1867.653290238478)   \n",
       "\n",
       "                                              grobid_text  \\\n",
       "0       Gabidulin codes [1] can be seen as the analogs...   \n",
       "1       For Gabidulin codes, there is no polynomial-ti...   \n",
       "2       In this paper, we provide a lower and an upper...   \n",
       "3       Let q be a power of a prime, let F q denote th...   \n",
       "4       . The (usual) addition and the non-commutative...   \n",
       "...                                                   ...   \n",
       "529298  Example 7.20.Consider the TRS R of f(s(x), 0) ...   \n",
       "529299  Here, however, we must not conclude linear run...   \n",
       "529300  All described techniques have been incorporate...   \n",
       "529301  Table 1 summarises the experimental results of...   \n",
       "529302  However if we consider RMIs upto dimension 3 t...   \n",
       "\n",
       "                                            pdf_alto_text  \\\n",
       "0       ['Gabidulin[~end_of_font~] codes[~end_of_font~...   \n",
       "1       ['For[~end_of_font~] Gabidulin[~end_of_font~] ...   \n",
       "2       ['In[~end_of_font~] this[~end_of_font~] paper,...   \n",
       "3       ['Let[~end_of_font~] q[~end_of_font~] be[~end_...   \n",
       "4       ['following[~end_of_font~] holds:[~end_of_font...   \n",
       "...                                                   ...   \n",
       "529298  ['Example[~end_of_font~] 7.20.[~end_of_font~] ...   \n",
       "529299  ['Here,[~end_of_font~] however,[~end_of_font~]...   \n",
       "529300  ['All[~end_of_font~] described[~end_of_font~] ...   \n",
       "529301  ['Table[~end_of_font~] 1[~end_of_font~] summar...   \n",
       "529302  ['However[~end_of_font~] if[~end_of_font~] we[...   \n",
       "\n",
       "                                                    fonts  \\\n",
       "0       ['font8  font8  font8  font8  font8  font8  fo...   \n",
       "1       ['font8  font8  font8  font8  font8  font8  fo...   \n",
       "2       ['font8  font8  font8  font8  font8  font8  fo...   \n",
       "3       ['font8  font10  font8  font8  font8  font8  f...   \n",
       "4       ['font8  font8  font10  font8  font21  font10 ...   \n",
       "...                                                   ...   \n",
       "529298  ['font28  font28  font4  font4  font4  font13 ...   \n",
       "529299  ['font4  font4  font4  font4  font4  font4  fo...   \n",
       "529300  ['font4  font4  font4  font4  font4  font4  fo...   \n",
       "529301  ['font4  font4  font4  font4  font4  font4  fo...   \n",
       "529302  ['font4  font4  font4  font4  font4  font4  fo...   \n",
       "\n",
       "                        label  \\\n",
       "0                       basic   \n",
       "1                       basic   \n",
       "2                       basic   \n",
       "3                       basic   \n",
       "4                       basic   \n",
       "...                       ...   \n",
       "529298  uri:extthm.example.25   \n",
       "529299  uri:extthm.example.25   \n",
       "529300                  basic   \n",
       "529301                  basic   \n",
       "529302                  basic   \n",
       "\n",
       "                                             global_fonts    Normal  \\\n",
       "0                                  [cmr10, cmti10, cmr10]  1.000000   \n",
       "1                                  [cmr10, cmmi10, cmr10]  1.000000   \n",
       "2                                                 [cmr10]  1.000000   \n",
       "3       [cmr10, cmmi10, cmr10, msbm10, cmmi8, cmr10, c...  0.792500   \n",
       "4       [cmr10, cmmi10, cmr10, cmr8, cmmi10, cmr10, cm...  0.940769   \n",
       "...                                                   ...       ...   \n",
       "529298  [cmbx10, cmr10, cmsy10, cmr10, cmss10, cmsy10,...  0.928480   \n",
       "529299                             [cmr10, cmsy10, cmr10]  1.000000   \n",
       "529300  [cmr10, cmti10, cmss10, cmr10, cmr8, cmr10, cm...  0.985119   \n",
       "529301      [cmr10, cmsy10, cmmi10, cmr10, cmss10, cmr10]  1.000000   \n",
       "529302  [cmr10, cmsy10, cmr10, cmmi10, cmr10, cmmi10, ...  1.000000   \n",
       "\n",
       "        Superscipt  ...  is_Serif  font_color_red  font_color_green  \\\n",
       "0         0.000000  ...       0.0             0.0               0.0   \n",
       "1         0.000000  ...       0.0             0.0               0.0   \n",
       "2         0.000000  ...       0.0             0.0               0.0   \n",
       "3         0.051528  ...       0.0             0.0               0.0   \n",
       "4         0.019231  ...       0.0             0.0               0.0   \n",
       "...            ...  ...       ...             ...               ...   \n",
       "529298    0.034483  ...       0.0             0.0               0.0   \n",
       "529299    0.000000  ...       0.0             0.0               0.0   \n",
       "529300    0.014881  ...       0.0             0.0               0.0   \n",
       "529301    0.000000  ...       0.0             0.0               0.0   \n",
       "529302    0.000000  ...       0.0             0.0               0.0   \n",
       "\n",
       "        font_color_blue  is_bold_manual  is_italic_manual  is_serif_manual  \\\n",
       "0                   0.0        0.000000          0.006944         0.000000   \n",
       "1                   0.0        0.000000          0.008929         0.000000   \n",
       "2                   0.0        0.000000          0.000000         0.000000   \n",
       "3                   0.0        0.000000          0.414306         0.000000   \n",
       "4                   0.0        0.000000          0.201795         0.000000   \n",
       "...                 ...             ...               ...              ...   \n",
       "529298              0.0        0.027778          0.084770         0.027778   \n",
       "529299              0.0        0.000000          0.000000         0.000000   \n",
       "529300              0.0        0.000000          0.047685         0.007937   \n",
       "529301              0.0        0.000000          0.027778         0.000000   \n",
       "529302              0.0        0.000000          0.041481         0.000000   \n",
       "\n",
       "        is_math_manual  new_font_size  \\\n",
       "0             0.000000       0.272725   \n",
       "1             0.000000       0.272725   \n",
       "2             0.000000       0.272725   \n",
       "3             0.052639       0.245332   \n",
       "4             0.000000       0.268373   \n",
       "...                ...            ...   \n",
       "529298        0.221871       0.267470   \n",
       "529299        0.071429       0.272725   \n",
       "529300        0.000000       0.271632   \n",
       "529301        0.006944       0.272725   \n",
       "529302        0.006579       0.272725   \n",
       "\n",
       "                                                 pdf_path  \n",
       "0       1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...  \n",
       "1       1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...  \n",
       "2       1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...  \n",
       "3       1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...  \n",
       "4       1205.0345/Wachter-Zeh_BoundsGabidulin_ACCT2012...  \n",
       "...                                                   ...  \n",
       "529298                                1102.3129/arxiv.pdf  \n",
       "529299                                1102.3129/arxiv.pdf  \n",
       "529300                                1102.3129/arxiv.pdf  \n",
       "529301                                1102.3129/arxiv.pdf  \n",
       "529302                                1102.3129/arxiv.pdf  \n",
       "\n",
       "[529303 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def fonts_in_single_para(val):\n",
    "    literal=ast.literal_eval(val)\n",
    "    combined_fonts=[font_line.strip() for font_line in literal]\n",
    "    combined_para_fonts=\" \".join(combined_fonts)\n",
    "    combined_para_fonts=combined_para_fonts.split()\n",
    "    return combined_para_fonts\n",
    "\n",
    "def continuous_fonts_clubbed(val):\n",
    "    \"\"\"clubs fonts that are together in a group sequence\"\"\"\n",
    "    fonts_unique_order=[]\n",
    "    for font in val:\n",
    "        try:\n",
    "            last_font=fonts_unique_order[-1]\n",
    "        except:\n",
    "            fonts_unique_order.append(font)\n",
    "            continue\n",
    "        if(font!=last_font):\n",
    "            fonts_unique_order.append(font)\n",
    "            \n",
    "    return fonts_unique_order\n",
    "\n",
    "\n",
    "val_table[\"global_fonts\"]=val_table[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "val_table[\"global_fonts\"]=val_table[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e312e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['msbm10',\n",
       " 'cmmi6',\n",
       " 'cmr6',\n",
       " 'cmr8',\n",
       " 'cmmi6',\n",
       " 'cmr6',\n",
       " 'cmr8',\n",
       " 'cmr10',\n",
       " 'cmss10',\n",
       " 'cmmi8',\n",
       " 'cmr10',\n",
       " 'cmsy10',\n",
       " 'cmr10',\n",
       " 'cmss10',\n",
       " 'cmr10',\n",
       " 'msbm10',\n",
       " 'cmr10',\n",
       " 'cmss10',\n",
       " 'cmr10']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_table[\"global_fonts\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "champion-brighton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_fonts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>529303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.795603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.860435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        global_fonts\n",
       "count  529303.000000\n",
       "mean       23.795603\n",
       "std        36.860435\n",
       "min         0.000000\n",
       "25%         5.000000\n",
       "50%        14.000000\n",
       "75%        29.000000\n",
       "max      1528.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.DataFrame(val_table[\"global_fonts\"].apply(len))\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designing-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_fonts</th>\n",
       "      <th>pdf_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7181</th>\n",
       "      <td>[nimbusromno9l, cmr10, cmsy10, cmmi10, cmmi7, ...</td>\n",
       "      <td>1806.00661/PIRCSI_Anoosheh_V4_longer_version.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98151</th>\n",
       "      <td>[cmr10, cmmi7, cmsy10, cmmi7, cmr10, nimbusrom...</td>\n",
       "      <td>1207.0334/gap.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146180</th>\n",
       "      <td>[nimbusromno9l, cmr10, nimbusromno9l, cmr10, n...</td>\n",
       "      <td>1802.09099/Pareto_mp_arxiv.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146181</th>\n",
       "      <td>[cmsy10, cmmi7, cmr10, cmsy10, cmmi7, cmr10, c...</td>\n",
       "      <td>1802.09099/Pareto_mp_arxiv.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146182</th>\n",
       "      <td>[cmr10, cmmi7, cmr10, cmmi10, cmsy7, cmmi7, cm...</td>\n",
       "      <td>1802.09099/Pareto_mp_arxiv.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146183</th>\n",
       "      <td>[nimbusromno9l, cmmi10, cmsy10, nimbusromno9l,...</td>\n",
       "      <td>1802.09099/Pareto_mp_arxiv.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253967</th>\n",
       "      <td>[nimbusromno9l, cmmi10, cmsy10, nimbusromno9l,...</td>\n",
       "      <td>1907.01133/ms.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268746</th>\n",
       "      <td>[nimbusromno9l, cmmi10, nimbusromno9l, cmsy10,...</td>\n",
       "      <td>1806.02127/ms.pdf</td>\n",
       "      <td>uri:extthm.proof.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316781</th>\n",
       "      <td>[nimbusromno9l, cmsy10, cmmi7, cmmi10, cmsy10,...</td>\n",
       "      <td>1701.03559/ThR_DPM_arXiv.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412946</th>\n",
       "      <td>[cmr10, cmr8, cmex7, cmbx5, cmsy5, cmmi5, cmr5...</td>\n",
       "      <td>1503.04661/FoundationsCollatz_Arxiv_ProofRead.pdf</td>\n",
       "      <td>basic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             global_fonts  \\\n",
       "7181    [nimbusromno9l, cmr10, cmsy10, cmmi10, cmmi7, ...   \n",
       "98151   [cmr10, cmmi7, cmsy10, cmmi7, cmr10, nimbusrom...   \n",
       "146180  [nimbusromno9l, cmr10, nimbusromno9l, cmr10, n...   \n",
       "146181  [cmsy10, cmmi7, cmr10, cmsy10, cmmi7, cmr10, c...   \n",
       "146182  [cmr10, cmmi7, cmr10, cmmi10, cmsy7, cmmi7, cm...   \n",
       "146183  [nimbusromno9l, cmmi10, cmsy10, nimbusromno9l,...   \n",
       "253967  [nimbusromno9l, cmmi10, cmsy10, nimbusromno9l,...   \n",
       "268746  [nimbusromno9l, cmmi10, nimbusromno9l, cmsy10,...   \n",
       "316781  [nimbusromno9l, cmsy10, cmmi7, cmmi10, cmsy10,...   \n",
       "412946  [cmr10, cmr8, cmex7, cmbx5, cmsy5, cmmi5, cmr5...   \n",
       "\n",
       "                                                 pdf_path               label  \n",
       "7181     1806.00661/PIRCSI_Anoosheh_V4_longer_version.pdf               basic  \n",
       "98151                                   1207.0334/gap.pdf               basic  \n",
       "146180                     1802.09099/Pareto_mp_arxiv.pdf               basic  \n",
       "146181                     1802.09099/Pareto_mp_arxiv.pdf               basic  \n",
       "146182                     1802.09099/Pareto_mp_arxiv.pdf               basic  \n",
       "146183                     1802.09099/Pareto_mp_arxiv.pdf               basic  \n",
       "253967                                  1907.01133/ms.pdf               basic  \n",
       "268746                                  1806.02127/ms.pdf  uri:extthm.proof.4  \n",
       "316781                       1701.03559/ThR_DPM_arXiv.pdf               basic  \n",
       "412946  1503.04661/FoundationsCollatz_Arxiv_ProofRead.pdf               basic  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#paragraphs that use more than thresh set unique/not unique types of different fonts\n",
    "df=val_table\n",
    "thresh=1000\n",
    "print(len(df[\"global_fonts\"][df[\"global_fonts\"].apply(len)>thresh]))\n",
    "more_number_of_font=df[df[\"global_fonts\"].apply(len)>thresh][[\"global_fonts\",\"pdf_path\",\"label\"]]\n",
    "more_number_of_font"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-devil",
   "metadata": {},
   "source": [
    "# train test split(As per NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frequent-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs=list(df[\"pdf_path\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "typical-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3682\n"
     ]
    }
   ],
   "source": [
    "print(len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-somewhere",
   "metadata": {},
   "source": [
    "## Problem1: No font sequences for some blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rocky-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_no                                                   4\n",
      "top_left            (866.6111111111111, 1885.8888888888887)\n",
      "bot_right           (934.2222222222222, 1909.8888888888887)\n",
      "grobid_text                                           where\n",
      "pdf_alto_text                                          ['']\n",
      "fonts                                                  ['']\n",
      "label                                      uri:extthm.thm.2\n",
      "global_fonts                                             []\n",
      "Normal                                                  0.0\n",
      "Superscipt                                              0.0\n",
      "Subscript                                               0.0\n",
      "italics                                                 0.0\n",
      "bold                                                    0.0\n",
      "is_Proportional                                         0.0\n",
      "is_Serif                                                0.0\n",
      "font_color_red                                          0.0\n",
      "font_color_green                                        0.0\n",
      "font_color_blue                                         0.0\n",
      "is_bold_manual                                          0.0\n",
      "is_italic_manual                                        0.0\n",
      "is_serif_manual                                         0.0\n",
      "is_math_manual                                          0.0\n",
      "new_font_size                                           0.0\n",
      "pdf_path             1305.2999/LTE_small_cell_in_GSM_v2.pdf\n",
      "Name: 43612, dtype: object\n",
      "total places in Total data that have no sequence information:1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i,element in enumerate(df[\"global_fonts\"]):\n",
    "    if(len(element)==0):\n",
    "        print(df.iloc[i])\n",
    "        count+=1\n",
    "        break\n",
    "print(\"total places in Total data that have no sequence information:{}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instrumental-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "page_no                                                   4\n",
      "top_left            (866.6111111111111, 1885.8888888888887)\n",
      "bot_right           (934.2222222222222, 1909.8888888888887)\n",
      "grobid_text                                           where\n",
      "pdf_alto_text                                          ['']\n",
      "fonts                                                  ['']\n",
      "label                                      uri:extthm.thm.2\n",
      "global_fonts                                             []\n",
      "Normal                                                  0.0\n",
      "Superscipt                                              0.0\n",
      "Subscript                                               0.0\n",
      "italics                                                 0.0\n",
      "bold                                                    0.0\n",
      "is_Proportional                                         0.0\n",
      "is_Serif                                                0.0\n",
      "font_color_red                                          0.0\n",
      "font_color_green                                        0.0\n",
      "font_color_blue                                         0.0\n",
      "is_bold_manual                                          0.0\n",
      "is_italic_manual                                        0.0\n",
      "is_serif_manual                                         0.0\n",
      "is_math_manual                                          0.0\n",
      "new_font_size                                           0.0\n",
      "pdf_path             1305.2999/LTE_small_cell_in_GSM_v2.pdf\n",
      "Name: 43612, dtype: object\n",
      "total places in val data that have no sequence information:1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i,element in enumerate(val_table[\"global_fonts\"]):\n",
    "    if(len(element)==0):\n",
    "        print(\"yo\")\n",
    "        print(val_table.iloc[i])\n",
    "        count+=1\n",
    "        break\n",
    "print(\"total places in val data that have no sequence information:{}\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-enzyme",
   "metadata": {},
   "source": [
    "## Problem2: More than 500 font sequences for some blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "undefined-verification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "page_no                                                             3\n",
      "top_left                                  (136.0, 154.94444444444443)\n",
      "bot_right                                (1564.0000000000002, 2084.0)\n",
      "grobid_text         2 , D (j) ).It is easy to see that each of the...\n",
      "pdf_alto_text       ['Proof.[~end_of_font~] The[~end_of_font~] pro...\n",
      "fonts               ['font10  font10  font10  font10  font10  font...\n",
      "label                                                           basic\n",
      "global_fonts        [nimbusromno9l, cmmi9, cmr9, cmsy9, nimbusromn...\n",
      "Normal                                                       0.832692\n",
      "Superscipt                                                   0.067653\n",
      "Subscript                                                    0.099654\n",
      "italics                                                           0.0\n",
      "bold                                                              0.0\n",
      "is_Proportional                                              0.989936\n",
      "is_Serif                                                          0.0\n",
      "font_color_red                                                    0.0\n",
      "font_color_green                                                  0.0\n",
      "font_color_blue                                                   0.0\n",
      "is_bold_manual                                               0.453554\n",
      "is_italic_manual                                             0.199733\n",
      "is_serif_manual                                              0.453554\n",
      "is_math_manual                                                0.07616\n",
      "new_font_size                                                0.216452\n",
      "pdf_path                          1902.09537/itw2018-camera-ready.pdf\n",
      "Name: 1781, dtype: object\n",
      "total places in data that have no sequence information:1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "thresh=500\n",
    "for i,element in enumerate(df[\"global_fonts\"]):\n",
    "    if(len(element)>=thresh):\n",
    "        print(\"yo\")\n",
    "        print(df.iloc[i])\n",
    "        count+=1\n",
    "        break\n",
    "print(\"total places in data that have no sequence information:{}\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f2d84",
   "metadata": {},
   "source": [
    "# building vocabulary ignore if already built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294eb5b8-af84-4e93-83bb-cc3f26ccf648",
   "metadata": {},
   "source": [
    "#we need to build the training vocabulary\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "top_words = 52000 \n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=top_words,lower=False,oov_token=\"UNK\")\n",
    "\n",
    "#traverse through all the training file csvs\n",
    "#df[\"global_fonts\"]=df[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "#df[\"global_fonts\"]=df[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "\n",
    "train_path=\"/Volumes/My_Book/Theoremkb/cluster_package/8-finetuning_data/train_data/**.csv\"\n",
    "\n",
    "files=glob.glob(train_path)\n",
    "\n",
    "for i,file in tqdm(enumerate(files)):\n",
    "    df=pd.read_csv(file)\n",
    "    df[\"global_fonts\"]=df[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "    df[\"global_fonts\"]=df[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "    tokenizer.fit_on_texts(df[\"global_fonts\"])\n",
    "    \n",
    "    \n",
    "\n",
    "#tokenizer.fit_on_texts(val_table[\"global_fonts\"])\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(val_table[\"global_fonts\"])\n",
    "\n",
    "#for bert max length is 512 \n",
    "#X_train = pad_sequences(list_tokenized_train, maxlen=max_length)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ea23f-d851-435b-93dc-b18da5bf993e",
   "metadata": {},
   "source": [
    "#save the tokenizer to be able to load it further\n",
    "print(len(tokenizer.word_counts))\n",
    "import pickle\n",
    "\n",
    "#fitting on validation data\n",
    "#val_table[\"global_fonts\"]=val_table[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "#val_table[\"global_fonts\"]=val_table[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "tokenizer.fit_on_texts(val_table[\"global_fonts\"])\n",
    "print(len(tokenizer.word_counts))\n",
    "\n",
    "\n",
    "#fitting on test data\n",
    "test_table[\"global_fonts\"]=test_table[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "test_table[\"global_fonts\"]=test_table[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "tokenizer.fit_on_texts(test_table[\"global_fonts\"])\n",
    "print(len(tokenizer.word_counts))\n",
    "\n",
    "\n",
    "#saving the model\n",
    "#filepath = \"tokenizer_52000.pkl\"\n",
    "#with open(filepath, 'wb') as f:\n",
    "    #pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0bfebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"tokenizer_52000_v1.pkl\"\n",
    "#with open(filepath, 'wb') as f:\n",
    "    #pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0eda01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:26:10.290425: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4031\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer construct validation dataset\n",
    "\n",
    "#read the tokenizer\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "filepath = \"tokenizer_52000_v1.pkl\"\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    print(len(tokenizer.word_counts))\n",
    "    \n",
    "max_length=1000\n",
    "\n",
    "val_tokenized_train = tokenizer.texts_to_sequences(val_table[\"global_fonts\"])\n",
    "X_val = pad_sequences(val_tokenized_train, maxlen=max_length)\n",
    "\n",
    "#test_tokenized_train = tokenizer.texts_to_sequences(test_table[\"global_fonts\"])\n",
    "#X_test = pad_sequences(test_tokenized_train, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc169348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(529303, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c7df494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529303\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  2 13  2]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_val))\n",
    "print(X_val[0])\n",
    "sample=X_val[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9959aa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK cmbx10 cmr10 cmti10 cmsy10 cmmi10 cmti10 msbm10 cmmi8 cmmi6 cmti10 cmmi10 cmsy10 cmmi10 cmti10 cmmi10 cmr10 cmmi10 cmsy10 cmmi10 cmr10 cmti10 cmmi10 cmti10 cmmi10 cmti10 cmmi10 cmti10 cmbx10 cmr10 cmr8 cmmi10 cmr8 cmmi10 cmmi8 cmr10 cmsy10 msbm10 cmmi8 cmmi6 cmti10']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([sample]) #if giving a single value then that has to be passed through the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a420e4-6efe-46a7-9ca9-2bdb9cfeed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the base model\n",
    "def setting_up_labels(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return \"basic\"\n",
    "    if(\"overlap\" in label):\n",
    "        return \"overlap\"\n",
    "    if(\"proof\" in label):\n",
    "        return \"proof\"\n",
    "    if(\"theorem\" in label):\n",
    "        return  \"theorem\"\n",
    "    else:\n",
    "        return \"theorem\"\n",
    "    \n",
    "    \n",
    "def setting_up_labels_1(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return 0\n",
    "    if(\"overlap\" in label):\n",
    "        return 3\n",
    "    if(\"proof\" in label):\n",
    "        return 1\n",
    "    if(\"theorem\" in label):\n",
    "        return  2\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc801f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly we can get y_train and y_test\n",
    "#y_test=test_table[\"label\"].to_numpy()\n",
    "val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels)\n",
    "val_table=val_table.dropna()\n",
    "val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels_1)\n",
    "y_val=val_table[\"label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdbf61ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(529303, 1000) (529303,)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c5864",
   "metadata": {},
   "source": [
    "## convert into dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a109443-99ea-4d59-bf1d-80b8a698044c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:27:23.201606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 12:27:25.650025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38155 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:14:00.0, compute capability: 8.0\n",
      "2023-04-20 12:27:25.674942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38280 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:15:00.0, compute capability: 8.0\n",
      "2023-04-20 12:27:25.676663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38280 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:39:00.0, compute capability: 8.0\n",
      "2023-04-20 12:27:25.678342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38280 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:3a:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "no of devices: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"no of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "devices = strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977b63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852aeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16 #originally 64\n",
    "\n",
    "batch_size=batch_size*devices\n",
    "#test_dataset=test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE).batch(batch_size)\n",
    "val_dataset=val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03580e",
   "metadata": {},
   "source": [
    "## Running LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5c1a73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_28.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_18.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_108.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_106.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_85.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_123.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_7.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_48.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_2.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_114.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_125.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_67.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_4.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_93.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_3.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_86.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_36.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_41.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_100.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_118.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_99.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_11.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_62.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_12.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_91.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_35.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_88.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_82.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_78.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_121.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_6.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_27.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_14.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_42.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_111.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_104.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_53.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_113.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_77.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_22.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_119.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_55.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_34.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_29.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_75.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_47.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_39.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_74.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_60.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_50.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_26.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_73.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_5.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_49.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_102.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_124.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_25.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_87.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_89.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_94.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_80.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_33.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_17.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_79.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_13.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_51.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_81.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_23.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_117.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_43.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_0.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_122.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_68.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_31.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_109.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_69.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_112.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_32.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_90.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_84.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_24.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_40.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_63.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_83.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_120.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_116.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_103.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_56.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_44.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_107.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_15.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_105.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_92.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_70.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_95.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_54.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_1.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_37.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_59.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_65.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_9.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_16.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_76.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_58.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_110.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_8.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_21.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_61.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_38.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_71.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_101.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_45.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_96.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_10.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_98.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_126.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_19.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_30.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_72.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_66.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_46.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_97.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_57.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_64.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_52.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_20.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_115.csv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/**.csv\"\n",
    "import glob\n",
    "files=glob.glob(train_path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1fc03ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          1664000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               82432     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,746,948\n",
      "Trainable params: 1,746,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#convert everything to tf dataset object\n",
    "#GRU's work worse than LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import  Dense, LSTM,GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow_addons.optimizers import AdamW ,LAMB\n",
    "# This is fixed.\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 32 ####hyper parameter\n",
    "learning_rate=0.001\n",
    "top_words = 52000 \n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#opt=AdamW(learning_rate)\n",
    "opt=LAMB(learning_rate)\n",
    "\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(top_words, EMBEDDING_DIM))\n",
    "    #model.add(SpatialDropout1D(0.1))\n",
    "\n",
    "    model.add(LSTM(128)) #for cudadnn set rec dropout as 0 Bidirectional()\n",
    "    #model.add(LSTM(64)) #for cudadnn set rec dropout as 0\n",
    "    #model.add(LSTM(100, dropout=0.2)) #for cudadnn set rec dropout as 0\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "360a4446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, None, 1000), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe5e1cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_28.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_18.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_7.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_2.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_0.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_108.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_106.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_85.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_123.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_48.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_114.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_125.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_67.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_4.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_93.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_3.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_86.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_36.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_41.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_100.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_118.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_99.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_11.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_62.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_12.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_91.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_35.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_88.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_82.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_78.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_121.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_6.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_27.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_14.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_42.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_111.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_104.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_53.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_113.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_77.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_22.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_119.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_55.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_34.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_29.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_75.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_47.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_39.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_74.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_60.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_50.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_26.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_73.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_5.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_49.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_102.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_124.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_25.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_87.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_89.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_94.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_80.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_33.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_17.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_79.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_13.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_51.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_81.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_23.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_117.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_43.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_122.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_68.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_31.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_109.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_69.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_112.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_32.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_90.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_84.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_24.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_40.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_63.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_83.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_120.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_116.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_103.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_56.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_44.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_107.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_15.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_105.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_92.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_70.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_95.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_54.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_1.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_37.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_59.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_65.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_9.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_16.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_76.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_58.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_110.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_8.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_21.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_61.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_38.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_71.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_101.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_45.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_96.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_10.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_98.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_126.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_19.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_30.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_72.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_66.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_46.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_97.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_57.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_64.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_52.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_20.csv',\n",
       " '/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_115.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reordering file name accroding to some predefined run (which is here nlp)\n",
    "\n",
    "file_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/roberta_from_scratch_ft.txt\"\n",
    "\n",
    "import os\n",
    "base=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/\"\n",
    "with open(file_path,\"r\") as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "start=[]\n",
    "for line in lines:\n",
    "    val=line.split(\",\")[0]\n",
    "    start.append(val)\n",
    "\n",
    "for file in files:\n",
    "    if(file in start):\n",
    "        continue\n",
    "    else:\n",
    "        start.append(file)\n",
    "        \n",
    "files=start\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "453bd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_run(log_file):\n",
    "    with open(log_file,\"r\") as f:\n",
    "        lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    files=[]\n",
    "    for line in lines:\n",
    "        line=line.replace(\"./training_data/\",\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/\")\n",
    "        files.append(line.split(\",\")[0])\n",
    "        \n",
    "    return files\n",
    "\n",
    "def append_last_run(val,log_file):\n",
    "    with open(log_file,\"a\") as f:\n",
    "        f.write(val)\n",
    "\n",
    "\n",
    "log_file=\"./font_clf_blstm_res.txt\"\n",
    "#get_last_run(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9224effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_2.csv\n",
      "not in previous runs, hence running on this batch\n",
      "doing-- /linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 14:43:14.992707: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 187398\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\026TensorSliceDataset:126\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 1000\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2928/2929 [============================>.] - ETA: 0s - loss: 0.7071 - sparse_categorical_accuracy: 0.7269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 14:52:01.825899: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1563 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/stream_executor/cuda/cuda_dnn.cc(1593): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "2023-04-20 14:52:01.826449: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1563 : UNKNOWN: CUDNN_STATUS_BAD_PARAM\n",
      "in tensorflow/stream_executor/cuda/cuda_dnn.cc(1593): 'cudnnSetTensorNdDescriptor( tensor_desc.get(), data_type, sizeof(dims) / sizeof(dims[0]), dims, strides)'\n",
      "3it [09:08, 182.72s/it]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node cond/output/_15/_138}}]] [Op:__inference_train_function_172513]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m df_dataset\u001b[38;5;241m=\u001b[39mdf_dataset\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m---> 38\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#model_res= model.evaluate(val_dataset)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#val_loss,val_acc=model_res[0],model_res[1]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#metrics after a batch\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: Graph execution error:\n\nRecvAsync is cancelled.\n\t [[{{node cond/output/_15/_138}}]] [Op:__inference_train_function_172513]"
     ]
    }
   ],
   "source": [
    "log_file=\"./bgru_font.txt\"\n",
    "model_path=\"./bgru_font.h5\"\n",
    "\n",
    "\n",
    "if(os.path.exists(model_path)):\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "#skips=[\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_93.csv\"]\n",
    "\n",
    "for i,file in tqdm(enumerate(files)):\n",
    "    #get readings from past run\n",
    "    try:\n",
    "        run_so_far=get_last_run(log_file)\n",
    "        if(file in run_so_far ): #or file in skips\n",
    "            continue\n",
    "        else:\n",
    "            print(file)\n",
    "            print(\"not in previous runs, hence running on this batch\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    print(\"doing--\", file)\n",
    "    df=pd.read_csv(file)\n",
    "    df[\"global_fonts\"]=df[\"global_fonts\"].apply(fonts_in_single_para)\n",
    "    df[\"global_fonts\"]=df[\"global_fonts\"].apply(continuous_fonts_clubbed)\n",
    "    df[\"label\"]=df[\"label\"].apply(setting_up_labels)\n",
    "    df=df.dropna()\n",
    "    df[\"label\"]=df[\"label\"].apply(setting_up_labels_1)\n",
    "    df_tokenized_train = tokenizer.texts_to_sequences(df[\"global_fonts\"])\n",
    "    df_x = pad_sequences(df_tokenized_train, maxlen=max_length)\n",
    "    df_y=df[\"label\"].to_numpy()\n",
    "    \n",
    "    \n",
    "    df_dataset = tf.data.Dataset.from_tensor_slices((df_x, df_y)).shuffle(buffer_size=1024)\n",
    "    df_dataset=df_dataset.prefetch(buffer_size=tf.data.AUTOTUNE).batch(batch_size)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        history = model.fit(df_dataset,validation_data=val_dataset,epochs=1)\n",
    "        #model_res= model.evaluate(val_dataset)\n",
    "        #val_loss,val_acc=model_res[0],model_res[1]\n",
    "        #metrics after a batch\n",
    "        train_loss=history.history['loss']\n",
    "        val_loss=history.history['val_loss']\n",
    "        train_acc=history.history['sparse_categorical_accuracy']\n",
    "        val_acc=history.history['val_sparse_categorical_accuracy']\n",
    "        \n",
    "        line_to_write=f'{file},{train_loss},{val_loss},{train_acc},{val_acc}\\n'\n",
    "        append_last_run(line_to_write,log_file)\n",
    "        \n",
    "        #saving the model\n",
    "        model.save(model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d8c30-b93e-4166-8fae-c5a4612c0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "batch_size=64\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset=val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE).batch(batch_size)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tensorflow_addons.optimizers import AdamW ,LAMB\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels=None\n",
    "for x, y in tqdm(val_dataset):\n",
    "    if(labels is None):\n",
    "        labels=y\n",
    "    else:\n",
    "        labels=np.concatenate([labels,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cd645-b10e-4a21-a67f-eb96f2437ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground truth        \n",
    "y_true=labels\n",
    "\n",
    "def evaluate_f1_for_tf_model(model_path,validation_dataset,y_true,show_confusion_report=True):\n",
    "    \n",
    "    # 4 A100 can do the job in\n",
    "\n",
    "    class_names=[\"Basic\",\"Proof\",\"Theorem\",\"Overlap\"]\n",
    "\n",
    "    # Wrap the loaded model inside the strategy scope to distribute it across the GPUs\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    #show model arch\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    #generating predictions\n",
    "    predictions=model.predict(validation_dataset)\n",
    "    \n",
    "    #generating predictions\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    if(show_confusion_report is True):\n",
    "        print('Confusion Matrix')\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "        \n",
    "    return f1_score(y_true,y_pred,average=\"macro\")\n",
    "\n",
    "model=\"./my_model_blstm_128.h5\"\n",
    "_f1_score=evaluate_f1_for_tf_model(model_path=model,validation_dataset=val_dataset,y_true=y_true)\n",
    "print(f\"f1 score of the {model} is {_f1_score}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f640c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "universal-myrtle",
   "metadata": {},
   "source": [
    "Model: \"sequential_16\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding_16 (Embedding)    (None, None, 32)          16000     \n",
    "                                                                 \n",
    " gru_13 (LSTM)              (None, 128)               82432     \n",
    "                                                                 \n",
    " dense_16 (Dense)            (None, 3)                 387       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 98,819\n",
    "Trainable params: 98,819\n",
    "Non-trainable params: 0\n",
    "Epoch 1/30\n",
    "2022-04-18 15:18:24.670030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "2022-04-18 15:18:24.902177: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "2022-04-18 15:18:25.139851: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "78000/78000 [==============================] - ETA: 0s - loss: 0.8603 - accuracy: 0.6539\n",
    "2022-04-18 15:53:48.616853: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "2022-04-18 15:53:48.680270: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "78000/78000 [==============================] - 2216s 28ms/step - loss: 0.8603 - accuracy: 0.6539 - val_loss: 0.8662 - val_accuracy: 0.6542\n",
    "Epoch 2/30\n",
    "78000/78000 [==============================] - 2283s 29ms/step - loss: 0.8335 - accuracy: 0.6549 - val_loss: 0.8500 - val_accuracy: 0.6539\n",
    "Epoch 3/30\n",
    "78000/78000 [==============================] - 2271s 29ms/step - loss: 0.8255 - accuracy: 0.6555 - val_loss: 0.8665 - val_accuracy: 0.6532\n",
    "Epoch 4/30\n",
    "78000/78000 [==============================] - 2354s 30ms/step - loss: 0.8334 - accuracy: 0.6526 - val_loss: 0.8407 - val_accuracy: 0.6541\n",
    "Epoch 5/30\n",
    "78000/78000 [==============================] - 2234s 29ms/step - loss: 0.8257 - accuracy: 0.6541 - val_loss: 0.8619 - val_accuracy: 0.6538    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./my_model_lstm_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cd449",
   "metadata": {},
   "source": [
    "bi directional part fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05aba1",
   "metadata": {},
   "source": [
    "Model: \"sequential_7\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding_7 (Embedding)     (None, 1000, 32)          16000     \n",
    "                                                                 \n",
    " spatial_dropout1d_7 (Spatia  (None, 1000, 32)         0         \n",
    " lDropout1D)                                                     \n",
    "                                                                 \n",
    " lstm_6 (LSTM)               (None, 100)               53200     \n",
    "                                                                 \n",
    " dense_7 (Dense)             (None, 3)                 303       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 69,503\n",
    "Trainable params: 69,503\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebc82c",
   "metadata": {},
   "source": [
    "#simple lstm\n",
    "\n",
    "19500/19500 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.7013\n",
    "2022-04-11 12:39:51.713498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "\n",
    "2022-04-11 12:39:51.772903: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
    "\n",
    "19500/19500 [==============================] - 5009s 256ms/step - loss: 0.7095 - accuracy: 0.7013 - val_loss: 0.6975 - val_accuracy: 0.7053\n",
    "\n",
    "Epoch 2/30\n",
    "19500/19500 [==============================] - 4937s 253ms/step - loss: 0.6849 - accuracy: 0.7105 - val_loss: 0.6912 - val_accuracy: 0.7085\n",
    "\n",
    "Epoch 3/30\n",
    "19500/19500 [==============================] - 5082s 261ms/step - loss: 0.6788 - accuracy: 0.7123 - val_loss: 0.6884 - val_accuracy: 0.7100\n",
    "\n",
    "Epoch 4/30\n",
    "10236/19500 [==============>...............] - ETA: 37:42 - loss: 0.6776 - accuracy: 0.7125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ff6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #embedding dimension\n",
    "#LSTM cells number\n",
    "#recurrent drop outs\n",
    "#dropouts\n",
    "#spatial dropouts\n",
    "\n",
    "#LSTM 600 of 32 gives acc or loss\n",
    "#bi directional improves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayesian optimization\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    #some initial things\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    batch_size=32\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Categorical parameter\n",
    "    EMBEDDING_DIM=trial.suggest_int(\"num_units\", 16, 768,log=True)\n",
    "    model.add(Embedding(top_words, EMBEDDING_DIM))\n",
    "    \n",
    "    #spatial dropout\n",
    "    spatial_dropout_rate = trial.suggest_float(\"spatial_dropout_rate\", 0.0, 1.0)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    \n",
    "    ## Floating point parameter dropout same as dropout rate\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 1.0)\n",
    "    \n",
    "    # Integer parameter\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        if(i!=n_layers-1):\n",
    "            num_hidden = trial.suggest_int(f'lstm_units_l{i}', 4, 512, log=True)\n",
    "            model.add(LSTM(num_hidden, dropout=dropout_rate,return_sequences=True)) #for cudadnn set rec dropout as 0\n",
    "        else:\n",
    "            model.add(LSTM(num_hidden, dropout=dropout_rate))\n",
    "        \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    epochs = 3 #hyper parameter not really that important\n",
    "\n",
    "    batch_size = 32 #number of training samples to push through\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                      validation_data=(X_val,y_val),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss',mode=\"min\",  patience=3, min_delta=0.01)]\n",
    "                      )\n",
    "    \n",
    "    return hist.history['val_loss'][-1]\n",
    "    \n",
    "# Creating a study object and optimizing the objective function.\n",
    "study = optuna.create_study(direction='minimize',study_name=\"NN_tuning\",storage=\"sqlite:///nn_net.db\",load_if_exists=True)\n",
    "# n_trials means number of trials\n",
    "study.optimize(objective, n_trials=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10547db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Again loading the object from the nn_net file and updating it again.\n",
    "study = optuna.create_study(direction='minimize',\n",
    "                            sampler=optuna.samplers.TPESampler(seed=5),\n",
    "                            study_name=\"NN_tuning\",\n",
    "                            storage=\"sqlite:///nn_net.db\",\n",
    "                            load_if_exists=True)\n",
    "import pandas as pd\n",
    "print(\"best_params\",study.best_params)\n",
    "print(study.best_value)\n",
    "print(\"best trial--\",study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "study=optuna.study.load_study(storage=\"sqlite:///nn_net.db\",study_name=\"NN_tuning\")\n",
    "import pandas as pd\n",
    "print(\"best_params\",study.best_params)\n",
    "print(study.best_value)\n",
    "print(\"best trial--\",study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history\n",
    "plot_optimization_history(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_parallel_coordinate\n",
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_param_importances\n",
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7c2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.1_py3.10",
   "language": "python",
   "name": "module-conda-env-tensorflow-2.9.1_py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
